{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#selenium이라는 크롤링 패키지를 설치한 후 불러오세요\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import re\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selenium 사용 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구글 창에 chromedriver라고 치시면 나오는 사이트에 들어가셔서 다운을 받습니다.\n",
    "버전은 상관 없지만 다운로드할 때 다운로드 폴더에 풀지마세요.\n",
    "다운로드 폴더에 풀면 이상하게 오류가 뜹니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실행하면 창 하나가 뜹니다.\n",
    "driver=webdriver.Chrome('C:/Users/user/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#뜬 창을 통해 크롤링하고 싶은 원하는 사이트에 들어가기 위해 해당 코드를 칩니다.\n",
    "driver.get('크롤링하고 싶은 원하는 사이트 링크')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크롤링할 때 일정한 속도로 크롤링을 진행하면 ip를 일정 시간 동안 막을 수도 있어서\n",
    "그 점을 보완하고자 속도를 조절합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.implicitly_wait(randint(2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "페이지의 이미지나 텍스트를 추출하기 위해서 여러 방법이 있는데 각 사이트에 동일한\n",
    "위치의 파일이 많을 경우, find_element_by_xpath라는 코드를 사용합니다.\n",
    "\n",
    "사용 방법은 두 가지가 있는데\n",
    "1. 해당 위치의 xpath를 복사하여 입력\n",
    "2. class나 tag 등 크게 잡아서 해당 하는 파일 위치를 찾아내는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#첫번째 방법\n",
    "driver.find_element_by_xpath(\"\"\"//*//*[@id=\"table-browse\"]/tbody/tr[{0}]/td[3]/a\"\"\".format(i))\n",
    "#두번째 방법\n",
    "a_element=driver.find_elements_by_xpath(\"//td[@class='firstcol'] |a[@href]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 코드 입력 후 바로 치면 이상한 문자가 나타남. 바로 쓸 수 없어서 다른 방법을\n",
    "사용해야 합니다. \n",
    "\n",
    " - df.text -> 해당 xpath의 text 파일을 보여주어라\n",
    " - df.screenshot -> 해당 xpath의 이미지를 다운로드해라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예시\n",
    "ic=[]\n",
    "for i in range(len(a_element)):\n",
    "    ic.append(a_element[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beautifulsoup 사용 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beautifulsoup은 링크로 넣어야 하는 걸로 알고 있습니다.... 저도 잘 몰라요\n",
    "\n",
    "그 후 html.parser이라는 걸 통해 html 관련 코드 및 텍스트 링크를 싹다 긁어 올 수 있습니다. 그 후 자신이 원하는 걸 추출해 낼 수 있습니다.findAll 함수를 사용해서요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests를 사용해도 되지만 그럼 또 close하기 귀찮아서 selenium에서 html링크를 가져옵니다.\n",
    "html=driver.page_source\n",
    "# 그 후 beautifulsoup을 사용해서 html 코드 및 텍스트, 링크를 가져 옵니다.\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "#이제 원하는 걸 가져와야 하는데 그 부분을 위해선 findAll함수를 사용합니다.\n",
    "data_list=soup.findAll('div',{'id':'prod_cntr_scoure'})\n",
    "#div는 ▶이거 다음에 나오는 걸 넣으시면 되고\n",
    "# 그 뒤는 그 밑에 있는 것들 <>이거 안에 있는 것들을 쓰면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 해도 우리가 원하는 것들을 다시 찾아야 합니다. 안그러면 너무 더러워서요.....\n",
    "\n",
    "그래서 또 findAll함수를 사용하는데 여기서 주의할점이 빈 리스트를 만들어서 \n",
    "추가해줄 때 append사용하면 2차원 형태로 리스트가 나오게 되서 귀찮아 집니다.\n",
    "그래서 extend를 사용하여 값을 추가해줍시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예사\n",
    "li_list=[]\n",
    "for data1 in data1_list:\n",
    "    li_list.extend(data1.findAll('img'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또 주의할 점!\n",
    "\n",
    "저렇게 해서 뽑았는데 원하는 요소만 또 뽑을려면 slicing을 할 수 있는데 str가 아니라서\n",
    "slicing이 안됩니다. 그러므로 str로 바꿔서 slicing해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예시\n",
    "li_list2=[]\n",
    "for x in li_list:\n",
    "    li_list2.append(str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제가 ewg크롤링했을 때 사용한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,1921,10):\n",
    "    driver.implicitly_wait(randint(2,4))\n",
    "    driver.get(\"\"\"https://www.ewg.org/skindeep/browse.php?category=facial_cleanser&&showmore=products&start={0}\"\"\".format(j))\n",
    "    for i in range(2,12):\n",
    "        driver.find_element_by_xpath(\"\"\"//*//*[@id=\"table-browse\"]/tbody/tr[{0}]/td[3]/a\"\"\".format(i)).click()\n",
    "        a_element=driver.find_elements_by_xpath(\"//td[@class='firstcol'] |a[@href]\")\n",
    "        ic=[]\n",
    "        for i in range(len(a_element)):\n",
    "            ic.append(a_element[i].text)\n",
    "        ic=[x for x in ic if x]\n",
    "        span_element=driver.find_elements_by_xpath(\"//td[@width]  |span[@style]\")\n",
    "        sb=[]\n",
    "        for i in range(len(span_element)):\n",
    "            sb.append(span_element[i].text)\n",
    "        sb=[x for x in sb if x]\n",
    "        div_element=driver.find_elements_by_xpath(\"//td[@align]  |div[@*]\")\n",
    "        ssb=[]\n",
    "        for i in range(len(div_element)):\n",
    "            ssb.append(div_element[i].text)\n",
    "        ssb=[x for x in ssb if x]\n",
    "        html=driver.page_source\n",
    "        soup=BeautifulSoup(html,'html.parser')\n",
    "        data1_list=soup.findAll('div',{'id':\"prod_cntr_score\"})\n",
    "        li_list=[]\n",
    "        for data1 in data1_list:\n",
    "            li_list.extend(data1.findAll('img'))\n",
    "        li_list2=[]\n",
    "        for x in li_list:\n",
    "            li_list2.append(str(x))\n",
    "        li_list3=[]\n",
    "        for y in range(len(li_list2)):\n",
    "            li_list3.append(li_list2[y][68:69])\n",
    "        ic=pd.Series(ic,name='ingredient')\n",
    "        sb=pd.Series(sb,name='concerns')\n",
    "        ssb=pd.Series(ssb,name='score')\n",
    "        li_list=pd.Series(li_list3,name='score_number')\n",
    "        first=pd.concat([ic,sb,ssb,li_list],axis=1)\n",
    "        ewg_facial=ewg_facial.append(first)\n",
    "        driver.back()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
